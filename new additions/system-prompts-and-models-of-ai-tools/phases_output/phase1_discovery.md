# Phase 1: Initial Discovery (Config: GEMINI_BASIC)

## Agent Findings

```json
{
  "phase": "Initial Discovery",
  "findings": [
    {
      "agent": "Structure Agent",
      "findings": "**Structure Agent Report: Project Layout Analysis**\n\n---\n\n### 1. Project Overview\n\nThe project exhibits a highly organized structure primarily serving as a repository for AI prompts and associated tool definitions. It appears to consolidate configurations and textual instructions for various AI agents, applications, or models, categorizing them into distinct, dedicated directories. The focus is clearly on managing and centralizing linguistic assets for AI systems.\n\n---\n\n### 2. Directory and File Organization\n\nThe project's organization is characterized by a flat hierarchy of top-level directories, each representing a specific AI agent, platform, or prompt category. This modular approach aims to encapsulate all relevant prompt and tool data for a given entity within its dedicated folder.\n\n*   **Top-Level Structure:**\n    *   The root contains numerous directories (e.g., `Cluely`, `Cursor Prompts`, `Devin AI`, `Replit`, `Same.dev`, `Windsurf`, etc.), each named after what appears to be an AI agent, tool, or a domain-specific collection of prompts.\n    *   A single `LICENSE.md` file resides at the root, indicating the project's licensing terms.\n\n*   **Common Directory Patterns:**\n    *   The most prevalent pattern is a directory containing one or more `.txt` files (identified as prompts) and, in several cases, one or more `.json` files (identified as tool definitions).\n    *   **Single Prompt Directories:** Many directories like `Devin AI`, `dia`, `Junie`, `Lovable`, `Spawn`, `Trae`, `v0 Prompts and Tools`, `VSCode Agent`, and `Warp.dev` contain a single `Prompt.txt` or `Chat Prompt.txt` file. This suggests a direct mapping of one directory to one primary prompt.\n    *   **Multiple Prompt/Variant Directories:**\n        *   `Cluely` contains `Default Prompt.txt` and `Enterprise Prompt.txt`, indicating different prompt versions or use-cases for the same agent.\n        *   `Cursor Prompts` is a notable example, housing multiple prompt files (`Agent Prompt.txt`, `Chat Prompt.txt`, `Memory Prompt.txt`, `Memory Rating Prompt.txt`) and even versioned prompts (`Agent Prompt v1.0.txt`, `Agent Prompt v1.2.txt`). This suggests a more granular management of prompts for this specific agent.\n    *   **Agent with Tools Directories:** Directories like `Manus Agent Tools & Prompt`, `Replit`, `Same.dev`, and `Windsurf` clearly separate the prompt from the associated tools, typically named `tools.json` or `Agent Tools vX.Y.json`.\n    *   **Nested Category Directories:** `Open Source prompts` serves as a meta-category, containing further sub-directories (`Bolt`, `Cline`, `Codex CLI`, `RooCode`), each with its own `Prompt.txt`. This indicates an additional layer of categorization for open-source prompt contributions or examples.\n\n*   **File Naming Conventions:**\n    *   **Prompts:** Files consistently use `.txt` extension and are named descriptively (e.g., `Prompt.txt`, `Chat Prompt.txt`, `Memory Prompt.txt`, `Agent loop.txt`, `Modules.txt`). Versioning is sometimes indicated in the filename (e.g., `v1.0`).\n    *   **Tools:** Files consistently use `.json` extension and are named `tools.json` or `Agent Tools vX.Y.json`, clearly indicating their purpose as machine-readable tool definitions.\n\n---\n\n### 3. File Relationships and Purpose\n\nThe project structure defines clear relationships between prompts and their corresponding agents/tools through the directory structure.\n\n*   **Prompt Files (`.txt`):**\n    *   These files are the core of the project, containing the natural language instructions, contexts, and examples designed to guide Large Language Models (LLMs) or AI agents.\n    *   Each prompt file is implicitly related to the agent or system represented by its parent directory. For instance, `Devin AI/Prompt.txt` is the main prompt for the \"Devin AI\" system.\n    *   Variations (e.g., `Default Prompt.txt` vs. `Enterprise Prompt.txt` in `Cluely`, or `Chat Prompt.txt` vs. `Memory Prompt.txt` in `Cursor Prompts`) suggest different operational modes or specialized functionalities of the same underlying agent.\n    *   Files like `Agent loop.txt` and `Modules.txt` (in `Manus Agent Tools & Prompt`) imply more complex prompt structures, possibly defining workflow logic or available modules for an agent.\n\n*   **Tool Definition Files (`.json`):**\n    *   These JSON files define the structure and capabilities of external tools, functions, or APIs that an AI agent can invoke. They likely conform to a specific schema (e.g., OpenAI function calling specification, LangChain tools).\n    *   They are directly linked to the agent defined by their containing directory. For example, `Replit/Tools.json` describes the tools available to the \"Replit\" agent.\n    *   The presence of these files alongside prompts indicates that many of these agents are not just conversational but are designed to interact with external systems or perform actions through defined interfaces.\n\n*   **Miscellaneous Files (`LICENSE.md`):**\n    *   The `LICENSE.md` file at the root provides essential legal information regarding the terms of use, distribution, and modification of the project's content. It applies to the entire collection of prompts and tools.\n\n---\n\n### 4. Key Architectural Components\n\nBased on the observed file organization, the following key architectural components can be identified:\n\n*   **Prompt Repository/Library:** The entire project functions as a centralized repository for managing and organizing various AI prompts. This is the primary architectural component, providing a structured way to store, retrieve, and potentially version control prompt definitions.\n*   **Agent Configurations:** Each top-level directory effectively serves as a configuration unit for a specific AI agent or application. These \"agent configuration packages\" encapsulate the necessary prompts and tools for that agent's operation.\n*   **Tool Manifests (or Tool Registries):** The `Tools.json` files act as manifests or registries for the capabilities of different agents. They define the callable functions and their parameters, enabling the AI models to interact with external services or execute specific logic.\n*   **Prompt Versioning/Variants:** Explicitly named versions (`v1.0`, `v1.2`) and contextual names (`Default`, `Enterprise`, `Chat`, `Memory`) for prompts within certain directories highlight a built-in mechanism for managing prompt evolution and variations, crucial for iterative AI development.\n*   **Categorization System:** The nested `Open Source prompts` directory demonstrates a hierarchical categorization system, allowing for grouping prompts by origin, type, or purpose.\n\n---\n\n### 5. Observations and Recommendations\n\n*   **Consistency:** The project demonstrates good consistency in its naming conventions (`Prompt.txt`, `Tools.json`) and the primary organization principle (one directory per agent/category). This greatly enhances navigability and understanding.\n*   **Modularity:** The clear separation of prompts and tools within distinct directories promotes modularity, making it easier to manage, update, or extend individual agent configurations without affecting others.\n*   **Scalability:** This structure is highly scalable for a collection of prompts and tools. Adding a new agent simply involves creating a new directory.\n*   **Readability:** The descriptive file and directory names contribute significantly to the project's readability.\n*   **Potential Improvements:**\n    *   **Schema Enforcement for JSON:** While `Tools.json` files are present, it's not clear if there's a defined schema enforced across all of them. Documenting or enforcing a consistent schema for tool definitions could improve robustness and interoperability.\n    *   **Metadata:** Consider adding a small metadata file (e.g., `config.json` or `info.json`) within each agent directory to include details like a brief description of the agent, its intended use, author, or specific LLM compatibility, rather than relying solely on directory names.\n    *   **Automated Testing/Validation:** For a prompt repository, implementing automated tests to validate prompt effectiveness or tool integration could be beneficial.\n\nIn summary, this project is meticulously organized as a specialized prompt and tool configuration repository, exhibiting a clear, intuitive structure that supports the development and management of diverse AI agents."
    },
    {
      "agent": "Dependency Agent",
      "findings": "**Dependency Agent Report**\n\n**Project Context Analysis:**\nThe provided project structure primarily consists of text files (`.txt`) and JSON data files (`.json`) organized into various directories, many of which are named after specific AI agents, platforms, or prompt collections (e.g., \"Cluely\", \"Cursor Prompts\", \"Devin AI\", \"Replit\", \"Same.dev\", \"Warp.dev\"). There are no explicit package manager files (e.g., `requirements.txt`, `package.json`, `pom.xml`, etc.) nor any code files (e.g., `.py`, `.js`, `.java`) that would typically declare traditional software library dependencies. The `package_info` field is empty, confirming the absence of declared dependencies.\n\n**Findings:**\n\n**1. Investigation of Packages and Libraries:**\nBased on the file types and organizational structure, this project does *not* contain traditional software packages or libraries in the sense of installable components that would be managed by a package manager. Instead, it appears to be a curated repository of:\n*   **AI Prompts:** Text files ending with `Prompt.txt` or similar (e.g., `Default Prompt.txt`, `Agent Prompt.txt`, `Chat Prompt.txt`). These are instructions or context provided to AI models.\n*   **AI Tool Definitions:** JSON files (e.g., `Agent Tools v1.0.json`, `tools.json`). These files likely define the structure and capabilities of tools that AI agents can use or call, conforming to specific API or schema requirements of various AI frameworks or platforms.\n\nTherefore, the \"dependencies\" are not external software libraries but rather:\n*   **Underlying AI Models/Platforms:** The prompts and tools are designed to interact with specific Large Language Models (LLMs) or AI agent orchestration frameworks.\n*   **Data Formats/Schemas:** The JSON tool definitions depend on a specific schema (e.g., OpenAI Functions schema, LangChain tools schema, custom internal schema) understood by the environment where the AI agent operates.\n\n**2. Version Requirements:**\nGiven the nature of the project, there are no explicit external library version requirements. However, internal versioning is evident:\n*   **Prompt/Tool Internal Versioning:** The presence of files like `Agent Prompt v1.0.txt`, `Agent Prompt v1.2.txt`, and `Agent Tools v1.0.json` indicates an internal version control for the prompts and tool definitions themselves. This suggests that the content or format of these files may change over time, requiring users to be aware of which version they are employing with a given AI system.\n*   **Implicit API/Schema Versioning:** The JSON `tools.json` files implicitly rely on the specific API version or schema expected by the AI framework integrating these tools. For example, if a tool definition adheres to an older or newer version of an OpenAI function calling schema, it might not work correctly with a different API version of the AI model or framework. Without knowing the target AI system, specific schema versions cannot be determined.\n\n**3. Compatibility Issues:**\nCompatibility issues for this project are conceptual and relate to the interaction between the prompts/tools and the AI ecosystem:\n\n*   **AI Model Compatibility:**\n    *   **Prompt Efficacy:** A prompt well-tuned for one LLM (e.g., GPT-4) might perform suboptimally or even fail to elicit the desired response from another LLM (e.g., Claude, Llama 2, or an older GPT version) due to differences in training data, architecture, safety filters, or instruction following capabilities.\n    *   **Context Window Limitations:** Different models have different context window sizes. Very long prompts might exceed the limits of some models.\n    *   **Instruction Following Nuances:** How strictly models adhere to instructions, their preferred output format, and their ability to follow complex multi-step reasoning can vary significantly.\n\n*   **AI Agent Framework/Platform Compatibility:**\n    *   **Tool Definition Schema:** The `tools.json` files must strictly adhere to the schema expected by the specific AI agent framework or platform executing these tools. For instance, an `Agent Tools v1.0.json` file designed for OpenAI's function calling might not be directly compatible with a LangChain `Tool` definition or a custom agent framework without conversion.\n    *   **Tool Implementation:** The tools defined in JSON files typically reference external functions or APIs. These underlying functions must be correctly implemented and accessible within the environment where the AI agent operates. If the referenced functions are missing, incorrectly named, or have incompatible arguments, the agent will fail to use the tools.\n    *   **Platform-Specific Optimizations:** Directory names like \"Devin AI\", \"Replit\", \"Same.dev\", \"Warp.dev\", and \"Windsurf\" suggest that the prompts and tools within these folders might be specifically designed, optimized, or even hardcoded for those respective platforms or their proprietary AI agents. Using them outside their intended environment could lead to unexpected behavior or require significant modifications.\n\n*   **Backward/Forward Compatibility (Prompts/Tools):**\n    *   The internal versioning (e.g., `v1.0` vs. `v1.2`) suggests that changes between versions of the same prompt or tool definition might not be backward compatible. An agent designed to work with `v1.0` might malfunction if given a `v1.2` prompt/tool without updates to its logic.\n\n**Recommendations:**\n\n1.  **Document Target AI Models/Platforms:** For each set of prompts and tools, explicitly document which AI models (e.g., GPT-4-turbo, Claude 3 Opus) and specific AI agent frameworks/platforms (e.g., OpenAI Assistants API, LangChain, Custom Agent Framework) they are designed for and tested with.\n2.  **Define Tool Schemas:** If not already defined, establish and document the JSON schema that `tools.json` files must conform to for each target AI agent framework. This will help ensure consistency and compatibility.\n3.  **Provide Usage Examples:** For each prompt/tool set, provide example code or configuration snippets demonstrating how to load and use them with the intended AI models and frameworks.\n4.  **Version Control of Prompts/Tools:** Continue and formalize the internal versioning of prompts and tools. Clearly document changes and breaking alterations between versions.\n5.  **Environment Setup Guidance:** For any tools that rely on external functions or APIs, provide clear instructions on how to set up the necessary environment (e.g., API keys, installed libraries for function implementations) for the AI agent to successfully utilize them.\n\nThis report concludes the dependency analysis for the given project context."
    },
    {
      "agent": "Tech Stack Agent",
      "findings": "**Tech Stack Report: AI Prompt and Tooling Repository Analysis**\n\nThis project appears to be a structured collection of prompts, agent configurations, and tool definitions primarily geared towards various AI agents and Large Language Models (LLMs). The focus is heavily on prompt engineering, AI agent design patterns, and integrating LLMs with external tools.\n\n---\n\n### **1. Identified Frameworks and Technologies**\n\nBased on the file structure and naming conventions, the following core technologies and concepts are identified:\n\n*   **Large Language Models (LLMs):** The foundational technology implicitly used, as the entire repository revolves around instructing and empowering AI models. Specific LLM providers (e.g., OpenAI, Google, Anthropic) are not named, but their underlying architectures are central.\n*   **Prompt Engineering:** The explicit and primary \"technology\" at play, involving the design, refinement, and organization of natural language instructions (`.txt` files) for AI models.\n*   **AI Agent Architectures:** The presence of \"Agent Prompt\", \"Agent Tools\", \"Agent loop.txt\", and \"Modules.txt\" points to the implementation or definition of autonomous/semi-autonomous AI agents that leverage LLMs and external capabilities.\n*   **Tool/Function Calling:** The `.json` files (e.g., `Agent Tools v1.0.json`, `tools.json`) are clear indicators of structured definitions for external tools or functions that LLMs can invoke. This aligns with modern LLM capabilities like OpenAI's Function Calling or Google's Gemini tool use.\n*   **JSON (JavaScript Object Notation):** Used as the standard data interchange format for defining AI agent tools and configurations, facilitating machine readability and interoperability.\n*   **Plain Text (.txt):** Utilized for storing human-readable prompts and configuration snippets.\n*   **Markdown (.md):** Standard format for documentation (e.g., `LICENSE.md`).\n\n**Inferred/Platform-Specific Technologies:**\n\nWhile not explicitly code, the directory names suggest integration with or targeting of specific AI development environments and tools:\n\n*   **Cursor:** An AI-powered code editor known for its advanced AI features and prompt-driven coding.\n*   **Devin AI:** An autonomous AI software engineer by Cognition Labs.\n*   **Replit (Ghostwriter/AI features):** A popular online IDE with integrated AI coding assistants.\n*   **Same.dev:** Likely an AI development platform specializing in agent and tool integration.\n*   **Vercel v0:** The \"v0 Prompts and Tools\" directory strongly suggests a connection to Vercel's generative UI tool (`v0.dev`), which creates UI components from text prompts.\n*   **VSCode Agent:** Implies an AI agent or extension specifically designed for Visual Studio Code.\n*   **Warp.dev:** A modern, AI-powered terminal that integrates AI features.\n*   **OpenAI Codex / CLI Tools:** \"Codex CLI\" might refer to a command-line interface leveraging an LLM like OpenAI's Codex for code generation or execution.\n*   **Generic AI Orchestration Frameworks (e.g., LangChain, LlamaIndex, LiteLLM):** While not directly named, the structured nature of agent tools and prompts implies that these assets are designed to be consumed by or adapted for such frameworks, which facilitate building complex LLM applications.\n\n---\n\n### **2. Latest Documentation (General Concepts & Best Practices)**\n\nSince specific versions of LLMs or frameworks are not indicated, the documentation links refer to the general concepts and common implementations.\n\n*   **Large Language Models (LLMs) General Concepts:**\n    *   **OpenAI API Documentation (Function Calling/Tools):** [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling) (Essential for understanding how LLMs can interact with tools defined in JSON.)\n    *   **Google Gemini API (Function Calling):** [https://ai.google.dev/docs/function_calling](https://ai.google.dev/docs/function_calling)\n    *   **Anthropic Claude API:** [https://docs.anthropic.com/claude/reference/getting-started](https://docs.anthropic.com/claude/reference/getting-started)\n*   **Prompt Engineering:**\n    *   **OpenAI Prompt Engineering Guide:** [https://platform.openai.com/docs/guides/prompt-engineering](https://platform.openai.com/docs/guides/prompt-engineering)\n    *   **DeepLearning.AI - Prompt Engineering for Developers Course:** [https://www.deeplearning.ai/short-courses/prompt-engineering-for-developer/](https://www.deeplearning.ai/short-courses/prompt-engineering-for-developer/)\n*   **AI Agent Architectures & Tooling:**\n    *   **LangChain Documentation (Agents & Tools):** [https://python.langchain.com/docs/modules/agents/](https://python.langchain.com/docs/modules/agents/) (A leading framework for building LLM-powered applications, including agents and tool usage.)\n    *   **LlamaIndex Documentation (Agents):** [https://docs.llamaindex.ai/en/stable/module_guides/applications/agents.html](https://docs.llamaindex.ai/en/stable/module_guides/applications/agents.html)\n*   **JSON Schema (for Tool Definitions):**\n    *   **Official JSON Schema Website:** [https://json-schema.org/](https://json-schema.org/) (While the project doesn't explicitly use JSON Schema definitions, understanding it is crucial for robust tool definitions.)\n*   **Specific Platform Documentation (where relevant directories exist):**\n    *   **Cursor Documentation:** [https://cursor.sh/docs](https://cursor.sh/docs)\n    *   **Replit Docs (AI features):** [https://docs.replit.com/ai/](https://docs.replit.com/ai/)\n    *   **Vercel v0 Documentation:** [https://v0.dev/docs](https://v0.dev/docs)\n    *   **Warp.dev Documentation (AI Commands):** [https://docs.warp.dev/getting-started/ai-commands](https://docs.warp.dev/getting-started/ai-commands)\n\n---\n\n### **3. Current Best Practices and Updates**\n\n*   **Prompt Engineering:**\n    *   **Clarity and Specificity:** Always aim for precise instructions. Avoid ambiguity.\n    *   **Role-Playing:** Define the AI's persona or role (e.g., \"You are an expert Python developer...\").\n    *   **Chain-of-Thought (CoT) Prompting:** Encourage the LLM to think step-by-step before answering. This can be induced by adding phrases like \"Let's think step by step.\" or by explicitly requesting intermediate thoughts.\n    *   **Few-Shot Examples:** Provide a few input-output examples to guide the model's behavior for specific tasks.\n    *   **Structured Output:** Explicitly request output in a specific format (e.g., JSON, XML) for easier parsing.\n    *   **System Messages:** For APIs that support it (like OpenAI's Chat Completion), use the \"system\" role to set the overall behavior and guardrails for the model, separating it from user input.\n    *   **Iterative Refinement:** Prompt engineering is an iterative process. Test, evaluate, and refine prompts based on results.\n    *   **Temperature and Top-P Settings:** Experiment with these parameters to control the creativity and determinism of the LLM's responses.\n*   **AI Agent Design:**\n    *   **Modularity:** Break down complex agent behaviors into smaller, manageable modules (as suggested by `Modules.txt`).\n    *   **Tool Integration:** Design tools with clear, concise descriptions and robust input/output schemas (JSON schema is highly recommended). Ensure tools handle errors gracefully.\n    *   **Memory Management:** Implement strategies for the agent to remember past interactions or relevant information (indicated by `Memory Prompt.txt`). This could involve vector databases, summarization, or simple context window management.\n    *   **Planning and Self-Correction:** Agents should be able to plan sequences of actions and, ideally, identify and correct mistakes.\n    *   **Observability:** Implement logging and monitoring to understand agent behavior and debug issues.\n    *   **Safety and Guardrails:** Crucial for agent deployments to prevent harmful or unintended actions, especially when tools can interact with external systems.\n*   **Tool Definition Best Practices (JSON):**\n    *   **Descriptive Naming:** Use clear, camelCase or snake_case names for tools and their parameters.\n    *   **Comprehensive Descriptions:** Provide detailed descriptions for each tool and its parameters, explaining their purpose and expected inputs/outputs. This helps the LLM understand when and how to use the tool.\n    *   **Schema Validation:** Define precise `type` and `properties` for parameters within the JSON structure (e.g., `type: \"string\"`, `type: \"integer\"`, `enum`, `required`).\n    *   **Version Control:** The project already demonstrates this with `v1.0`, `v1.2`. Continue to version prompts and tools, especially as their behavior or schema changes, to manage updates and potential rollbacks.\n*   **Updates in the LLM Ecosystem (Current Trends):**\n    *   **Multimodality:** LLMs increasingly support inputs beyond text (images, audio, video) and generate multimodal outputs.\n    *   **Increased Context Windows:** Models are capable of processing much longer inputs, reducing the need for complex summarization for context.\n    *   **Specialized Models:** Growth of fine-tuned or domain-specific models for particular tasks (e.g., code generation, medical analysis).\n    *   **RAG (Retrieval Augmented Generation):** Combining LLMs with external knowledge bases (like vector databases) to provide more accurate and up-to-date information.\n    *   **Open-Source LLMs:** A proliferation of powerful open-source models (e.g., Llama, Mistral) that can be self-hosted, offering more control and cost efficiency.\n    *   **Agentic Workflows:** The trend towards more autonomous agents that can execute complex tasks by chaining tools and reasoning steps, often in a loop.\n\n---\n\nThis project serves as an excellent resource or starting point for anyone working on AI agent development, prompt engineering, and LLM application building. The clear organization of prompts and tool definitions by agent or platform facilitates reusability and understanding."
    }
  ]
}
```
